{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Ordenación\n",
    "\n",
    "El problema de ordenar un conjunto de datos (por ejemplo, en orden ascendente) tiene gran importancia tanto teórica como práctica. En esta sección veremos principalmente algoritmos que ordenan mediante comparaciones entre llaves, para los cuales se puede demostrar una cota inferior que coincide con la cota superior provista por varios algoritmos. También veremos un algoritmo de otro tipo, que al no hacer comparaciones, no está sujeto a esa cota inferior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cota inferior\n",
    "\n",
    "Supongamos que deseamos ordenar tres datos $A$, $B$ y $C$. La siguiente figura muestra un árbol de decisión posible para resolver este problema. Los nodos internos del árbol representan comparaciones y los nodos externos representan salidas emitidas por el programa.\n",
    "\n",
    "![arbol-decision-ordenacion](recursos/arbol-decision-ordenacion.gif)\n",
    "\n",
    "Como se vio en el capítulo de búsqueda, todo árbol de decisión con $H$ hojas tiene al menos altura $\\log_2{H}$, y la altura del árbol de decisión es igual al número de comparaciones que se efectúan en el peor caso.\n",
    "\n",
    "En un árbol de decisión para ordenar $n$ datos se tiene que $H=n!$, y por lo tanto se tiene que todo algoritmo que ordene $n$ datos mediante comparaciones entre llaves debe hacer al menos $\\log_2{n!}$ comparaciones en el peor caso.\n",
    "\n",
    "Usando la aproximación de Stirling, se puede demostrar que $\\log_2{n!} = n \\log_2{n} + \\Theta(n)$, por lo cual la cota inferior es de $\\Theta(n\\log{n})$.\n",
    "\n",
    "Si suponemos que todas las posibles permutaciones resultantes son equiprobables, es posible demostrar también que el número promedio de comparaciones que cualquier algoritmo debe hacer es también de $\\Theta(n\\log{n})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quicksort\n",
    "\n",
    "Este método fue inventado por C.A.R. Hoare a comienzos de los '60s, y sigue siendo el método más eficiente para uso general.\n",
    "\n",
    "Quicksort es un ejemplo clásico de la aplicación del principio de *dividir para reinar*. Su estructura es la siguiente:\n",
    "\n",
    "* Primero se elige un elemento al azar, que se denomina el pivote.\n",
    "\n",
    "* El arreglo a ordenar se reordena dejando a la izquierda a los elementos menores que el pivote, el pivote al medio, y a la derecha los elementos mayores que el pivote:\n",
    "\n",
    "![particion](recursos/particion.gif)\n",
    "\n",
    "* Luego cada sub-arreglo se ordena recursivamente.\n",
    "\n",
    "La recursividad termina, en principio, cuando se llega a sub-arreglos de tamaño cero o uno, los cuales trivialmente ya están ordenados. En la práctica veremos que es preferible detener la recursividad antes de eso, para no desperdiciar tiempo ordenando recursivamente arreglos pequeños, los cuales pueden ordenarse más eficientemente usando Ordenación por Inserción, por ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quicksort(a):\n",
    "    qsort(a,0,len(a)-1)\n",
    "\n",
    "def qsort(a,i,j): # ordena a[i],...,a[j]\n",
    "    if i<j: # quedan 2 o más elementos por ordenar\n",
    "        k=particion(a,i,j)\n",
    "        qsort(a,i,k-1)\n",
    "        qsort(a,k+1,j)\n",
    "\n",
    "def particion(a,i,j): # particiona a[i],...,a[j], retorna posición del pivote\n",
    "    k=np.random.randint(i,j) # genera un número al azar k en rango i..j\n",
    "    (a[i],a[k])=(a[k],a[i]) # mueve a[k] al extremo izquierdo\n",
    "    # a[i] es el pivote\n",
    "    s=i # invariante: a[i+1..s]<=a[i], a[s+1..t]>a[i]\n",
    "    for t in range(s,j):\n",
    "        if a[t+1]<=a[i]:\n",
    "            (a[s+1],a[t+1])=(a[t+1],a[s+1])\n",
    "            s=s+1\n",
    "    # mover pivote al centro\n",
    "    (a[i],a[s])=(a[s],a[i])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chequea_orden(a):\n",
    "    print(\"Ordenado\" if np.all(a[:-1]<=a[1:]) else \"Desordenado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21939313 0.41324672 0.99870279 0.15140399 0.33302848 0.92304672\n",
      " 0.11206401 0.63418302 0.64504804 0.50401626 0.2064129  0.81939607]\n",
      "Desordenado\n",
      "[0.11206401 0.15140399 0.2064129  0.21939313 0.33302848 0.41324672\n",
      " 0.50401626 0.63418302 0.64504804 0.81939607 0.92304672 0.99870279]\n",
      "Ordenado\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.random(12)\n",
    "print(a)\n",
    "chequea_orden(a)\n",
    "quicksort(a)\n",
    "print(a)\n",
    "chequea_orden(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costo promedio de Quicksort\n",
    "\n",
    "Si suponemos, como una primera aproximación, que el pivote siempre resulta ser la mediana del conjunto, entonces el costo de ordenar está dado (aproximadamente) por la ecuación de recurrencia\n",
    "\n",
    "$$\n",
    "T(n)=n+2T\\left( \\frac{n}{2} \\right)\n",
    "$$\n",
    "\n",
    "Esto tiene solución $T(n) = n \\log_2{n}$ y es, en realidad, el *mejor* caso de Quicksort.\n",
    "\n",
    "Para analizar el tiempo promedio que demora la ordenación mediante Quicksort, observemos que el funcionamiento de Quicksort puede graficarse mediante un *árbol de partición*:\n",
    "\n",
    "![arbol-particion](recursos/arbol-particion.gif)\n",
    "\n",
    "Por la forma en que se construye, es fácil ver que el árbol de partición es un *árbol de búsqueda binaria*, y como el pivote es escogido al azar, entonces la raíz de cada subárbol puede ser cualquiera de los elementos del conjunto en forma equiprobable. En consecuencia, los árboles de partición y los árboles de búsqueda binaria tienen exactamente la misma distribución.\n",
    "\n",
    "En el proceso de partición, cada elemento de los subárboles ha sido comparado contra la raíz (el pivote). Al terminar el proceso, cada elemento ha sido comparado contra todos sus ancestros. Si sumamos todas estas comparaciones, el resultado total es igual al *largo de caminos internos*.\n",
    "\n",
    "Usando todas estas correspondencias, tenemos que, usando los resultados ya conocidos para árboles, el número promedio de comparaciones que realiza Quicksort es de:\n",
    "\n",
    "$$\n",
    "T(n)=1.38 n\\log_2{n}+\\Theta(n)\n",
    "$$\n",
    "\n",
    "Por lo tanto, Quicksort, en el caso esperado, corre en un tiempo proporcional a la cota inferior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peor caso de Quicksort\n",
    "\n",
    "El peor caso de Quicksort se produce cuando el pivote resulta ser siempre el mínimo o el máximo del conjunto. En este caso la ecuación de recurrencia es\n",
    "\n",
    "$$\n",
    "T(n) = n - 1 + T(n-1)\n",
    "$$\n",
    "            \n",
    "lo que tiene solución $T(n) = \\Theta(n^2)$. Desde el punto de vista del árbol de partición, esto corresponde a un árbol en \"zig-zag\".\n",
    "\n",
    "Si bien este peor caso es extremadamente improbable si el pivote se escoge al azar, algunas implementaciones de Quicksort toman como pivote al primer elemento del arreglo (suponiendo que, al venir el arreglo al azar, entonces el primer elemento es tan aleatorio como cualquier otro). El problema es que si el conjunto viene en realidad ordenado, entonces caemos justo en el peor caso cuadrático.\n",
    "\n",
    "Lo anterior refuerza la importancia de que el pivote se escoja al azar. Esto no aumenta significativamente el costo total, porque el número total de elecciones de pivote es $\\Theta(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejoras a Quicksort\n",
    "\n",
    "Quicksort puede ser optimizado de varias maneras, pero hay que ser muy cuidadoso con estas mejoras, porque es fácil que terminen empeorando el desempeño del algoritmo.\n",
    "\n",
    "En primer lugar, es desaconsejable hacer cosas que aumenten la cantidad de trabajo que se hace dentro del \"loop\" de partición, porque este es el lugar en donde se concentra el costo $\\Theta(n \\log{n})$.\n",
    "\n",
    "Algunas de las mejoras que han dado buen resultado son las siguientes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quicksort con \"mediana de 3\"\n",
    "\n",
    "En esta variante, el pivote no se escoge como un elemento tomado al azar, sino que primero se extrae una muestra de 3 elementos, y entre ellos se escoge a la mediana de esa muestra como pivote.\n",
    "\n",
    "Si la muestra se escoge tomando al primer elemento del arreglo, al del medio y al último, entonces lo que era el peor caso (arreglo ordenado) se transforma de inmediato en mejor caso.\n",
    "\n",
    "De todas formas, es aconsejable que la muestra se escoja al azar, y en ese caso el análisis muestra que el costo esperado para ordenar n elementos es\n",
    "\n",
    "$$\n",
    "\\frac{12}{7} n \\ln{n}  \\approx  1.19 n \\log_2{n}\n",
    "$$\n",
    "\n",
    "Esta reducción en el costo se debe a que el pivote es ahora una mejor aproximación a la mediana. De hecho, si en lugar de escoger una muestra de tamaño 3, lo hiciéramos con tamaños como 7, 9, etc., se lograría una reducción aún mayor, acercándonos cada vez más al óptimo, pero con rendimientos rápidamente decrecientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso de Ordenación por Inserción para ordenar sub-arreglos pequeños\n",
    "\n",
    "Tal como se dijo antes, no es eficiente ordenar recursivamente sub-arreglos demasiado pequeños.\n",
    "\n",
    "En lugar de esto, se puede establecer un tamaño mínimo $M$, de modo que los sub-arreglos de tamaño menor que esto se ordenan por inserción en lugar de por Quicksort.\n",
    "\n",
    "Claramente debe haber un valor óptimo para $M$, porque si creciera indefinidamente se llegaría a un algoritmo cuadrático. Esto se puede analizar, y el óptimo es cercano a 10.\n",
    "\n",
    "Como método de implementación, al detectarse un sub-arreglo de tamaño menor que $M$, se lo puede dejar simplemente sin ordenar, retornando de inmediato de la recursividad. Al final del proceso, se tiene un arreglo cuyos pivotes están en orden creciente, y encierran entre ellos a bloques de elementos desordenados, pero que ya están en el grupo correcto. Para completar la ordenación, entonces, basta con hacer una sola gran pasada de Ordenación por Inserción, la cual ahora no tiene costo $\\Theta(n^2)$, sino $\\Theta(nM)$, porque ningún elemento está a distancia mayor que $M$ de su ubicación definitiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordenar recursivamente sólo el sub-arreglo más pequeño\n",
    "\n",
    "Un problema potencial con Quicksort es la profundidad que puede llegar a tener la recursividad. En el peor caso, ésta puede llegar a ser $\\Theta(n)$.\n",
    "\n",
    "Para evitar esto, se puede usar recursividad solo para la \"mitad\" más pequeña, y ordenar la otra \"mitad\" de manera iterativa. Con este enfoque, cada llamada recursiva se aplica a un sub-arreglo cuyo tamaño es a lo más la mitad del tamaño del arreglo a ordenar, de modo que si llamamos $S(n)$ a la profundidad de recursión, tenemos que\n",
    "\n",
    "$$\n",
    "S(n) \\le 1 + S\\left(\\frac{n}{2}\\right)\n",
    "$$\n",
    "\n",
    "lo cual tiene solución $\\log_2{n}$, de modo que la profundidad de la recursión nunca es más que logarítmica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qsort(a,i,j): # ordena a[i],...,a[j]\n",
    "    while i<j: # quedan 2 o más elementos por ordenar\n",
    "        k=particion(a,i,j)\n",
    "        if k-i<=j-k: #mitad izquierda es más pequeña\n",
    "            qsort(a,i,k-1)\n",
    "            i=k+1\n",
    "        else:\n",
    "            qsort(a,k+1,j)\n",
    "            j=k-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66656624 0.76798384 0.87811887 0.74763235 0.30180187 0.82056957]\n",
      "[0.30180187 0.66656624 0.74763235 0.76798384 0.82056957 0.87811887]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.random(6)\n",
    "print(a)\n",
    "quicksort(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un algoritmo de selección basado en Quicksort\n",
    "\n",
    "Es posible modificar el algoritmo de Quicksort para seleccionar el $k$-ésimo elemento de un arreglo $a[0],\\ldots,a[n-1]$, esto es, el elemento que quedaría en la posición $a[k]$ si el arreglo se ordenara. Una solución trivial sería, precisamente, ordenar el arreglo y retornar ese elemento, pero la pregunta es si eso se puede hacer de manera más eficiente.\n",
    "\n",
    "Una idea para resolver este problema idea es ejecutar Quicksort, pero en lugar de ordenar las dos mitades, hacerlo solo con aquella mitad en donde se encontraría el elemento buscado, o incluso no hacer nada si tenemos suerte y el pivote quedó justo en la posición $a[k]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickselect(a,k):\n",
    "    assert k>=0 and k<len(a)\n",
    "    qselect(a,k,0,len(a)-1)\n",
    "    return a[k]\n",
    "\n",
    "def qselect(a,k,i,j): # selecciona el elemento que quedaría en a[k]\n",
    "                      # si se ordenara a[i],...,a[j] (i<=k<=j)\n",
    "    if i<j:\n",
    "        p=particion(a,i,j)\n",
    "        if p!=k:\n",
    "            if k<p:\n",
    "                qselect(a,k,i,p-1)\n",
    "            else:\n",
    "                qselect(a,k,p+1,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19052461 0.578469   0.14702632 0.62380414 0.21373454 0.59978106]\n",
      "k=4\n",
      "0.5997810643996624\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.random(6)\n",
    "print(a)\n",
    "k=int(input(\"k=\"))\n",
    "print(quickselect(a,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad la recursividad de ``qselect`` es fácil de transformar en iteración, y una vez hecho eso ya no hay razón para que sea una función aparte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickselect(a,k):\n",
    "    assert k>=0 and k<len(a)\n",
    "    i=0\n",
    "    j=len(a)-1\n",
    "    while i<j:\n",
    "        p=particion(a,i,j)\n",
    "        if p==k:\n",
    "            break\n",
    "        if k<p:\n",
    "            j=p-1\n",
    "        else:\n",
    "            i=p+1\n",
    "    return a[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28147833 0.06014974 0.56597127 0.42582274 0.66869136 0.11621771]\n",
      "k=3\n",
      "0.4258227414342961\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.random(6)\n",
    "print(a)\n",
    "k=int(input(\"k=\"))\n",
    "print(quickselect(a,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análisis del costo esperado de Quickselect es complicado, pero se puede demostrar que es $\\Theta(n)$. Por otra parte, el peor caso se da cuando el pivote cae siempre en un extremo, y en ese caso el costo es $\\Theta(n^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un algoritmo de selección de tiempo lineal en el peor caso\n",
    "\n",
    "La razón por la cual el algoritmo ``Quickselect`` puede demorar tiempo cuadrático en el peor caso es que el pivote puede resultar ser siempre el mínimo o el máximo, o un elemento muy cercano a los extremos.\n",
    "\n",
    "Esta debilidad se subsanaría si pudiéramos garantizar que los elementos que quedan a cada lado del pivote son siempre al menos una cierta fracción fija del total, digamos $\\alpha n$, para algún $\\alpha \\in (0,\\frac12)$.\n",
    "\n",
    "Una forma de lograrlo es la siguiente. Dividamos el conjunto completo de $n$ elementos en grupos de tamaño $5$, y encontremos la mediana de cada uno de estos pequeños conjuntos. Cada uno de ellos se puede visualizar como un orden parcial de la forma\n",
    "\n",
    "![spider](recursos/spider.png)\n",
    "\n",
    "donde los dos elementos de arriba son mayores que la mediana, y ésta es mayor que los dos de abajo.\n",
    "\n",
    "A continuación, aplicamos este mismo algoritmo, recursivamente, para encontrar la **mediana de las medianas**. Este elemento, que se muestra encerrado en un círculo en el diagrama siguiente, será nuestro pivote:\n",
    "\n",
    "![median-of-medians](recursos/median-of-medians.png)\n",
    "\n",
    "Como se ve, todos los elementos encerrados en las líneas punteadas están garantizados de estar por sobre o por debajo del pivote, respectivamente.\n",
    "Por lo tanto, en el peor caso, podemos garantizar que al menos $\\frac{3}{10}n$ elementos son menores que el pivote, otros $\\frac{3}{10}n$ son mayores que él.\n",
    "\n",
    "Por lo tanto, en el peor caso, la llamada recursiva que se hace para continuar el proceso se aplica a un conjunto de a lo más $\\frac{7}{10}n$ elementos.\n",
    "\n",
    "Si llamamos $T(n)$ al tiempo que demora este algoritmo en el peor caso, y si $cn$ es el tiempo que demora encontrar las medianas de los $n/5$ subconjuntos, para alguna constante $c$, entonces tenemos que, en el peor caso,\n",
    "\n",
    "$$\n",
    "T(n)=cn+T\\left(\\frac{n}{5}\\right)+T\\left(\\frac{7n}{10}\\right)\n",
    "$$\n",
    "\n",
    "Esta ecuación tiene solución lineal. En efecto, supongamos que existe una constante $d$ tal que $T(n)=dn$. Reemplazando en la ecuación, tenemos\n",
    "\n",
    "$$\n",
    "dn=cn+\\frac{dn}{5}+\\frac{7dn}{10}\n",
    "$$\n",
    "\n",
    "de donde podemos despejar $d$ y obtener $d=10c$. Por lo tanto, $T(n)=10cn$, y el algoritmo demora tiempo $\\Theta(n)$ en el peor caso.\n",
    "\n",
    "La razón por la cual la ecuación admite solución lineal es porque los coeficientes de $n$ en las llamadas recursivas suman $\\frac{1}{5}+\\frac{7}{10}<1$. Eso explica por qué no habría funcionado haber dividido el conjunto en grupos de tamaño 3. En ese caso, los coeficientes habrían sido $\\frac{1}{3}$ y $\\frac{2}{3}$ respectivamente, y su suma no sería menor que $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heapsort\n",
    "\n",
    "A partir de cualquier implementación de una cola de prioridad es posible obtener un algoritmo de ordenación. El esquema del algoritmo es:\n",
    "\n",
    "* Comenzar con una cola de prioridad vacía.\n",
    "* *Fase de construcción de la cola de prioridad*:\n",
    "Traspasar todos los elementos del conjunto que se va a ordenar a la cola de prioridad, mediante $n$ inserciones.\n",
    "* *Fase de ordenación*:\n",
    "Sucesivamente extraer el máximo $n$ veces. Los elementos van apareciendo en orden decreciente y se van almacenando en el conjunto de salida.\n",
    "\n",
    "Si aplicamos esta idea a las dos implementaciones simples de colas de prioridad, utilizando lista enlazada ordenada y lista enlazada desordenada, se obtienen los algoritmos de ordenación por Inserción y por Selección, respectivamente. Ambos son algoritmos cuadráticos, pero es posible que una mejor implementación lleve a un algoritmo más rápido. En el capítulo de *Pilas y Colas* vimos que una forma de obtener una implementación eficiente de colas de prioridad es utilizando una estructura de datos llamada *heap*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de Heapsort\n",
    "\n",
    "Al utilizar un heap como implementación de una cola de prioridad para construir un algoritmo de ordenación, se obtiene un algoritmo llamado *Heapsort*, para el cual resulta que tanto la fase de construcción de la cola de prioridad, como la fase de ordenación, tienen ambas costo $\\Theta(n \\log{n})$, de modo que el algoritmo completo tiene ese mismo costo.\n",
    "\n",
    "Por lo tanto, Heapsort tiene un orden de magnitud que coincide con la cota inferior, esto es, es óptimo incluso en el peor caso. Nótese que esto no era así para Quicksort, el cual era óptimo en promedio, pero no en el peor caso.\n",
    "\n",
    "De acuerdo a la descripción de esta familia de algoritmos, daría la impresión de que en la fase de construcción del heap se requeriría un arreglo aparte para el heap, distinto del arreglo de entrada. De la misma manera, se requeriría un arreglo de salida aparte, distinto del heap, para recibir los elementos a medida que van siendo extraídos en la fase de ordenación.\n",
    "\n",
    "En la práctica, esto no es necesario y basta con un sólo arreglo: todas las operaciones pueden efectuarse directamente sobre el arreglo de entrada.\n",
    "\n",
    "En primer lugar, en cualquier momento de la ejecución del algoritmo, los elementos se encuentran particionados entre aquellos que están ya o aún formando parte del heap, y aquellos que se encuentran aún en el conjunto de entrada, o ya se encuentran en el conjunto de salida, según sea la fase. Como ningún elemento puede estar en más de un conjunto a la vez, es claro que, en todo momento, en total nunca se necesita más de $n$ casilleros de memoria, si la implementación se realiza bien.\n",
    "\n",
    "En el caso de Heapsort, durante la fase de construcción del heap, podemos utilizar las celdas de la izquierda del arreglo para ir \"armando\" el heap. Las celdas necesarias para ello se las vamos \"quitando\" al conjunto de entrada, el cual va perdiendo elementos a medida que van \"trepando\" en el heap. Al concluir esta fase, todos los elementos han sido insertados, y el arreglo completo es un solo gran heap.\n",
    "\n",
    "![heap-cons](recursos/heap-cons.gif)\n",
    "\n",
    "En la fase de ordenación, se van extrayendo elementos del heap, con lo cual éste se contrae de tamaño y deja espacio libre al final, el cual puede ser justamente ocupado para ir almacenando los elementos a medida que van saliendo del heap (recordemos que van apareciendo en orden decreciente).\n",
    "\n",
    "![heap-ord](recursos/heap-ord.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización de la fase de construcción del heap\n",
    "\n",
    "Como se ha señalado anteriormente, tanto la fase de construcción del heap como la de ordenación demoran tiempo $\\Theta(n \\log{n})$. Esto es el mínimo posible (en orden de magnitud), de modo que no es posible mejorarlo significativamente.\n",
    "\n",
    "Sin embargo, es posible modificar la implementación de la fase de construcción del heap para que sea mucho más eficiente.\n",
    "\n",
    "La idea es invertir el orden de las \"mitades\" del arreglo, haciendo que el \"input\" esté a la izquierda y el \"heap\" a la derecha.\n",
    "\n",
    "En realidad, si el \"heap\" está a la derecha, entonces no es realmente un heap, porque no es un árbol completo (le falta la parte superior), pero sólo nos interesa que en ese sector del arreglo se cumplan las relaciones de orden entre cada padre y sus hijos. En cada iteración, se toma el último elemento del \"input\" y se le \"hunde\" dentro del heap de acuerdo a su nivel de prioridad.\n",
    "\n",
    "![heap-cons2](recursos/heap-cons2.gif)\n",
    "\n",
    "Al concluir, se llega igualmente a un heap completo, pero el proceso es significativamente más rápido.\n",
    "\n",
    "La razón es que, al ser \"hundido\", un elemento paga un costo proporcional a su distancia al fondo del árbol. Dada las características de un árbol, la gran mayoría de los elementos están al fondo o muy cerca de él, por lo cual pagan un costo muy bajo. En un análisis aproximado, la mitad de los elementos pagan 0 (ya están al fondo), la cuarta parte paga 1, la octava parte paga 2, etc. Sumando todo esto, tenemos que el costo total está acotado por\n",
    "\n",
    "$$\n",
    "n \\sum_{i\\ge 0}{\\frac{i}{2^{i+1}}}\n",
    "$$\n",
    "\n",
    "lo cual es igual a $n$.\n",
    "\n",
    "En la práctica, al comenzar a construir el heap \"de abajo hacia arriba\", no hace falta examinar los elementos que no tienen hijos, porque ellos cumplen automáticamente la relación de orden. Por lo tanto, basta comenzar a examinar los elementos desde el casillero $\\lfloor \\frac{n}{2} \\rfloor-1$ hacia atrás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hundir(a,j,n): # El elemento a[j] se hunde hasta su nivel de prioridad\n",
    "    while 2*j+1<n: # mientras tenga al menos 1 hijo\n",
    "        k=2*j+1 # el hijo izquierdo\n",
    "        if k+1<n and a[k+1]>a[k]: # el hijo derecho existe y es mayor\n",
    "            k+=1\n",
    "        if a[j]>=a[k]: # tiene mejor prioridad que ambos hijos\n",
    "            break\n",
    "        (a[j],a[k])=(a[k],a[j]) # se intercambia con el mayor de los hijos\n",
    "        j=k # bajamos al lugar del mayor de los hijos\n",
    "\n",
    "def heapsort(a):\n",
    "    n=len(a)\n",
    "    # Fase de construcción del heap\n",
    "    for i in range(n//2-1,-1,-1):\n",
    "        hundir(a,i,n)\n",
    "    # Fase de ordenación\n",
    "    for i in range(n-1,-1,-1):\n",
    "        (a[0],a[i])=(a[i],a[0])\n",
    "        hundir(a,0,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58123276 0.57772293 0.22690173 0.49691097 0.78430617 0.16308228]\n",
      "[0.16308228 0.22690173 0.49691097 0.57772293 0.58123276 0.78430617]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.random(6)\n",
    "print(a)\n",
    "heapsort(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radix Sort\n",
    "\n",
    "Los métodos anteriores operan mediante comparaciones de llaves, y están sujetos, por lo tanto, a la cota inferior $\\Omega(n \\log{n})$. Veremos a continuación el método *Radix Sort* (llamado también *Bucket Sort*), que opera de una manera distinta, y logra ordenar el conjunto en tiempo lineal.\n",
    "\n",
    "Supongamos que queremos ordenar $n$ números, cada uno de ellos compuesto de $k$ dígitos decimales. El siguiente es un ejemplo con $n=10$, $k=5$.\n",
    "\n",
    "```\n",
    "    73895\n",
    "    93754\n",
    "    82149\n",
    "    99046\n",
    "    04853\n",
    "    94171\n",
    "    54963\n",
    "    70471\n",
    "    80564\n",
    "    66496\n",
    "```\n",
    "\n",
    "Para comenzar, veamos cómo podemos ordenar el conjunto, en una pasada, si la llave de ordenación fuera uno solo de los dígitos, por ejemplo el tercero de izquierda a derecha (lo mostramos separado para destacarlo):\n",
    "\n",
    "```\n",
    "    99 0 46\n",
    "    82 1 49\n",
    "    94 1 71\n",
    "    70 4 71\n",
    "    66 4 96\n",
    "    80 5 64\n",
    "    93 7 54\n",
    "    73 8 95\n",
    "    04 8 53\n",
    "    54 9 63\n",
    "```\n",
    "\n",
    "La forma de hacerlo es la siguiente. Llamemos $j$ a la posición del dígito mediante el cual se ordena. La ordenación se puede hacer utilizando una cola de entrada, que contiene al conjunto a ordenar, y un arreglo de 10 colas de salida, subindicadas de 0 a 9. Los elementos se van sacando de la cola de entrada y se van encolando en la cola de salida $Q[d_j]$, donde $d_j$ es el $j$-ésimo dígito del elemento que se está transfiriendo.\n",
    "\n",
    "\n",
    "![bucket-pass](recursos/bucket-pass.gif)\n",
    "\n",
    "Al terminar este proceso, los elementos están separados por dígito. Para completar la ordenación, basta concatenar las $k$ colas de salida y formar nuevamente una sola cola con todos los elementos.\n",
    "\n",
    "Este proceso se hace en una pasada sobre los datos, y toma tiempo $\\Theta(n)$.\n",
    "\n",
    "Para ordenar el conjunto por las llaves completas, ejecutamos este proceso dígito por dígito, *de derecha a izquierda*, en cada pasada separando los elementos según el valor del dígito respectivo, luego recolectándolos para formar una sola cola, y realimentando el proceso con esos mismos datos. El conjunto completo queda finalmente ordenado (¡esto *no* es obvio!).\n",
    "\n",
    "Como hay que realizar $k$ pasadas y cada una de ellas toma tiempo $\\Theta(n)$, el tiempo total es $\\Theta(kn)$, que es el tamaño total del archivo de entrada (en bytes). Por lo tanto, la ordenación toma tiempo lineal en el tamaño de los datos.\n",
    "\n",
    "El proceso anterior se puede generalizar para cualquier alfabeto, no sólo dígitos (por ejemplo, ASCII). Esto aumenta el número de colas de salida, pero no cambia sustancialmente el tiempo que demora el programa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El tiempo es lineal, pero ¿es $\\Theta(n)$?\n",
    "\n",
    "Como $k$ es un parámetro ($k=5$ en nuestro ejemplo), es tentador decir que $\\Theta(kn)=\\Theta(n)$, pero en realidad no es tan simple.\n",
    "\n",
    "Si fijamos $k$, entonces el máximo número de elementos distintos que podemos generar es $10^k$. Por lo tanto, $n\\le 10^k$, es decir, $k\\ge \\log_{10}{n}$.\n",
    "\n",
    "Por lo tanto, $k$ no puede ser constante, sino que tiene que crecer logarítmicamente con $n$, de modo que el algoritmo en realidad demora tiempo $\\Omega(n\\log{n})$.\n",
    "\n",
    "### Archivos con records de largo variable\n",
    "\n",
    "Si las líneas a ordenar no son todas del mismo largo, es posible alargarlas hasta completar el largo máximo, con lo cual el algoritmo anterior es aplicable. Pero si hay algunas pocas líneas desproporcionadamente largas y otras muy cortas, se puede perder mucha eficiencia.\n",
    "\n",
    "Es posible, aunque no lo vamos a ver aquí, generalizar este algoritmo para ordenar líneas de largo variable sin necesidad de alargarlas. El resultado es que la ordenación se sigue realizando en tiempo proporcional al tamaño del archivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mergesort\n",
    "\n",
    "Si tenemos dos arreglos que ya están ordenados, podemos mezclarlos para formar un solo arreglo ordenado en tiempo proporcional a la suma de los tamaños de los dos arreglos.\n",
    "\n",
    "Esto se hace leyendo el primer elemento de cada arreglo, copiando hacia un arreglod de salida al menor de los dos, y avanzando al siguiente elemento en el arreglo respectivo. Cuando uno de los dos arreglos se termina, todos los elementos restantes del otro se copian hacia la salida. Este proceso se denomina \"mezcla\", o bien \"merge\", por su nombre en inglés.\n",
    "\n",
    "Como cada elemento se copia sólo una vez, y con cada comparación se copia algún elemento, es evidente que el costo de mezclar los dos archivos es lineal.\n",
    "\n",
    "Si bien es posible realizar el proceso de mezcla de dos arreglos contiguos *in situ*, el algoritmo es muy complicado y no resulta práctico. Por esta razón, el proceso se implementa generalmente copiando desde dos arreglos de entrada a uno de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(a,b):\n",
    "    i=0\n",
    "    j=0\n",
    "    while i<len(a) or j<len(b):\n",
    "        if j>=len(b) or (i<len(a) and a[i]<=b[j]):\n",
    "            yield a[i]\n",
    "            i=i+1\n",
    "        else:\n",
    "            yield b[j]\n",
    "            j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 43, 55, 57, 88, 91]\n",
      "[10, 17, 40, 61, 70, 76]\n",
      "[10, 17, 24, 40, 43, 55, 57, 61, 70, 76, 88, 91]\n"
     ]
    }
   ],
   "source": [
    "a = [24,43,55,57,88,91]\n",
    "b = [10,17,40,61,70,76]\n",
    "print(a)\n",
    "print(b)\n",
    "c=[x for x in merge(a,b)]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando esta idea en forma reiterada, es posible ordenar un conjunto. Una forma de ver esto es recursivamente, de manera \"*top-down*\", usando \"dividir para reinar\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergesort(a):\n",
    "    n=len(a)\n",
    "    if n>1:    \n",
    "        mergesort(a[0:n//2])\n",
    "        mergesort(a[n//2:n])\n",
    "        a[0:n]=[x for x in merge(a[0:n//2],a[n//2:n])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98208828 0.87903465 0.49512148 0.9665445  0.75927717 0.20751891]\n",
      "[0.20751891 0.49512148 0.75927717 0.87903465 0.9665445  0.98208828]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.random(6)\n",
    "print(a)\n",
    "mergesort(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tiempo total está dado aproximadamente por la ecuación de recurrencia\n",
    "\n",
    "$$\n",
    "T(n) = 2 T\\left(\\frac{n}{2}\\right) + n\n",
    "$$\n",
    "\n",
    "la cual tiene solución $\\Theta(n \\log{n})$, de modo que el algoritmo resulta ser óptimo.\n",
    "\n",
    "Esto mismo se puede implementar en forma no recursiva, \"*bottom-up*\", agrupando los elementos de a dos y mezclándolos para formar pares ordenados. Luego mezclamos pares para formar cuádruplas ordenadas, y así sucesivamente hasta mezclar las últimas dos mitades y formar el conjunto completo ordenado. Como cada \"ronda\" tiene costo lineal y se realizan $\\log{n}$ rondas, el costo total es $\\Theta(n \\log{n})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"*Runs*\"\n",
    "\n",
    "En el enfoque \"*bottom-up*\" no recursivo, en lugar de comenzar mezclando elementos individuales para formar pares, etc., podemos comenzar detectando \"*runs*\" (corridas), que son secuencias de elementos consecutivos lo más largas posibles que ya vengan ordenados. A partir de ahí, mezclamos \"*runs*\" consecutivas y lo seguimos haciendo hasta que quede solo una gran \"*run*\", que es el arreglo completo ordenado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mergesort para ordenamiento externo\n",
    "\n",
    "La idea de Mergesort es la base de la mayoría de los métodos de ordenamiento externo, esto es, métodos que ordenan conjuntos almacenados en archivos muy grandes, en donde no es posible copiar todo el contenido del archivo a memoria para aplicar alguno de los métodos estudiados anteriormente.\n",
    "\n",
    "En las implementaciones prácticas de estos métodos, se utiliza el enfoque no recursivo, optimizado usando las siguientes ideas:\n",
    "\n",
    "* No se comienza con elementos individuales para formar pares ordenados, sino que se generan archivos ordenados lo más grandes posibles. Para esto, el archivo de entrada se va leyendo por trozos a memoria y se ordena mediante Quicksort, por ejemplo.\n",
    "* En lugar de mezclar sólo dos archivos se hace una mezcla múltiple (con $k$ archivos de entrada. Como en cada iteración hay $k$ candidatos a ser el siguiente elemento en salir, y siempre hay que extrar al mínimo de ellos y sustituirlo en la lista de candidatos por su sucesor, la estructura de datos apropiada para ello es un heap.\n",
    "En caso que no baste con una pasada de mezcla múltiple para ordenar todo el archivo, el proceso se repite las veces que sea necesario.\n",
    "\n",
    "Al igual que en los casos anteriores, el costo total es $\\Theta(n \\log{n})$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
